{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/manikantha4576/Fmml__IIIT__H_Assignmentss/blob/main/Module_01_Lab_02_MLPractice.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Eu9VZbF01eq"
      },
      "source": [
        "# Machine learning terms and metrics\n",
        "\n",
        "FMML Module 1, Lab 2<br>\n",
        "\n",
        "\n",
        " In this lab, we will show a part of the ML pipeline by extracting features, training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8qBvyEem0vLi"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "# set randomseed\n",
        "rng = np.random.default_rng(seed=42)"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u3t59g5s1HfC"
      },
      "source": [
        "In this lab, we will use the California Housing dataset. There are 20640 samples, each with 8 attributes like income of the block, age of the houses per district etc. The task is to predict the cost of the houses per district.\n",
        "\n",
        "Let us download and examine the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8LpqjN991GGJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6004646-5beb-429a-9323-540a41fb8a62"
      },
      "source": [
        " dataset =  datasets.fetch_california_housing()\n",
        " # print(dataset.DESCR)  # uncomment this if you want to know more about this dataset\n",
        " # print(dataset.keys())  # if you want to know what else is there in this dataset\n",
        " dataset.target = dataset.target.astype(np.int) # so that we can classify\n",
        " print(dataset.data.shape)\n",
        " print(dataset.target.shape)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(20640, 8)\n",
            "(20640,)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-2-60ae2e9a125e>:4: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
            "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
            "  dataset.target = dataset.target.astype(np.int) # so that we can classify\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iNx4174W5xRg"
      },
      "source": [
        "Here is a function for calculating the 1-nearest neighbours"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07zpydQj1hIQ"
      },
      "source": [
        "def NN1(traindata, trainlabel, query):\n",
        "  diff  = traindata - query  # find the difference between features. Numpy automatically takes care of the size here\n",
        "  sq = diff*diff # square the differences\n",
        "  dist = sq.sum(1) # add up the squares\n",
        "  label = trainlabel[np.argmin(dist)] # our predicted label is the label of the training data which has the least distance from the query\n",
        "  return label\n",
        "\n",
        "def NN(traindata, trainlabel, testdata):\n",
        "  # we will run nearest neighbour for each sample in the test data\n",
        "  # and collect the predicted classes in an array using list comprehension\n",
        "  predlabel = np.array([NN1(traindata, trainlabel, i) for i in testdata])\n",
        "  return predlabel"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "03JktkfIGaje"
      },
      "source": [
        "We will also define a 'random classifier', which randomly allots labels to each sample"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fogWAtjyGhAH"
      },
      "source": [
        "def RandomClassifier(traindata, trainlabel, testdata):\n",
        "  # in reality, we don't need these arguments\n",
        "\n",
        "  classes = np.unique(trainlabel)\n",
        "  rints = rng.integers(low=0, high=len(classes), size=len(testdata))\n",
        "  predlabel = classes[rints]\n",
        "  return predlabel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Hjf1KHs7fU5"
      },
      "source": [
        "Let us define a metric 'Accuracy' to see how good our learning algorithm is. Accuracy is the ratio of the number of correctly classified samples to the total number of samples. The higher the accuracy, the better the algorithm."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouuCqWU07bz-"
      },
      "source": [
        "def Accuracy(gtlabel, predlabel):\n",
        "  assert len(gtlabel)==len(predlabel), \"Length of the groundtruth labels and predicted labels should be the same\"\n",
        "  correct = (gtlabel==predlabel).sum() # count the number of times the groundtruth label is equal to the predicted label.\n",
        "  return correct/len(gtlabel)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4vJFwBFa9Klw"
      },
      "source": [
        "Let us make a function to split the dataset with the desired probability."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ko0VzpSM2Tdi"
      },
      "source": [
        "def split(data, label, percent):\n",
        "  # generate a random number for each sample\n",
        "  rnd = rng.random(len(label))\n",
        "  split1 = rnd<percent\n",
        "  split2 = rnd>=percent\n",
        "  split1data = data[split1,:]\n",
        "  split1label = label[split1]\n",
        "  split2data = data[split2,:]\n",
        "  split2label = label[split2]\n",
        "  return split1data, split1label, split2data, split2label"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AcK3LEAJ_LGC"
      },
      "source": [
        "We will reserve 20% of our dataset as the test set. We will not change this portion throughout our experiments"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bBZkHBLJ1iU-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e2576c4b-c8aa-4c4f-a7b1-9b473800b087"
      },
      "source": [
        "testdata, testlabel, alltraindata, alltrainlabel = split(dataset.data, dataset.target, 20/100)\n",
        "print('Number of test samples = ', len(testlabel))\n",
        "print('Number of other samples = ', len(alltrainlabel))\n",
        "print('Percent of test data = ', len(testlabel)*100/len(dataset.target),'%')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of test samples =  4144\n",
            "Number of other samples =  16496\n",
            "Percent of test data =  20.07751937984496 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a6Ss0Z6IAGNV"
      },
      "source": [
        "## Experiments with splits\n",
        "\n",
        "Let us reserve some of our train data as a validation set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WFew2iry_7W7"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "60hiu4clFN1i"
      },
      "source": [
        "What is the accuracy of our classifiers on the train dataset?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DBlZDTHUFTZx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99973c38-e46a-47fb-8c08-2c2b628e8ea7"
      },
      "source": [
        "trainpred = NN(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using nearest neighbour is \", trainAccuracy)\n",
        "\n",
        "trainpred = RandomClassifier(traindata, trainlabel, traindata)\n",
        "trainAccuracy = Accuracy(trainlabel, trainpred)\n",
        "print(\"Train accuracy using random classifier is \", trainAccuracy)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy using nearest neighbour is  1.0\n",
            "Train accuracy using random classifier is  0.164375808538163\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h08-9gJDtSy"
      },
      "source": [
        "For nearest neighbour, the train accuracy is always 1. The accuracy of the random classifier is close to 1/(number of classes) which is 0.1666 in our case.\n",
        "\n",
        "Let us predict the labels for our validation set and get the accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4h7bXoW_2H3v",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d14976b9-e445-4c47-8681-d99cf742e6bd"
      },
      "source": [
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using nearest neighbour is \", valAccuracy)\n",
        "\n",
        "valpred = RandomClassifier(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy using random classifier is \", valAccuracy)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy using nearest neighbour is  0.34108527131782945\n",
            "Validation accuracy using random classifier is  0.1688468992248062\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "py9bLguFEjfg"
      },
      "source": [
        "Validation accuracy of nearest neighbour is considerably less than its train accuracy while the validation accuracy of random classifier is the same. However, the validation accuracy of nearest neighbour is twice that of the random classifier.\n",
        "\n",
        "Now let us try another random split and check the validation accuracy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujm3cyYzEntE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4aa4ba8-e92a-4f88-ad8b-bfd23562f67a"
      },
      "source": [
        "traindata, trainlabel, valdata, vallabel = split(alltraindata, alltrainlabel, 75/100)\n",
        "valpred = NN(traindata, trainlabel, valdata)\n",
        "valAccuracy = Accuracy(vallabel, valpred)\n",
        "print(\"Validation accuracy of nearest neighbour is \", valAccuracy)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Validation accuracy of nearest neighbour is  0.34048257372654156\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oSOx7U83EKie"
      },
      "source": [
        "You can run the above cell multiple times to try with different random splits.\n",
        "We notice that the accuracy is different for each run, but close together.\n",
        "\n",
        "Now let us compare it with the accuracy we get on the test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PNEZ5ToYBEDW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dd5314a7-2c06-4b2f-d8b0-2211bbc27226"
      },
      "source": [
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "testAccuracy = Accuracy(testlabel, testpred)\n",
        "print('Test accuracy is ', testAccuracy)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3dGD531K3gH"
      },
      "source": [
        "### Try it out for yourself and answer:\n",
        "1. How is the accuracy of the validation set affected if we increase the percentage of validation set? What happens when we reduce it?\n",
        "2. How does the size of the train and validation set affect how well we can predict the accuracy on the test set using the validation set?\n",
        "3. What do you think is a good percentage to reserve for the validation set so that thest two factors are balanced?\n",
        "\n",
        "Answer for both nearest neighbour and random classifier. You can note down the values for your experiments and plot a graph using  <a href=https://matplotlib.org/stable/gallery/lines_bars_and_markers/step_demo.html#sphx-glr-gallery-lines-bars-and-markers-step-demo-py>plt.plot<href>. Check also for extreme values for splits, like 99.9% or 0.1%"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PnYvkAZLQY7h"
      },
      "source": [
        "## Multiple Splits\n",
        "\n",
        "One way to get more accurate estimates for the test accuracy is by using <b>crossvalidation</b>. Here, we will try a simple version, where we do multiple train/val splits and take the average of validation accuracies as the test accuracy estimation. Here is a function for doing this. Note that this function will take a long time to execute."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E4nGCUQXBTzo"
      },
      "source": [
        "# you can use this function for random classifier also\n",
        "def AverageAccuracy(alldata, alllabel, splitpercent, iterations, classifier=NN):\n",
        "  accuracy = 0\n",
        "  for ii in range(iterations):\n",
        "    traindata, trainlabel, valdata, vallabel = split(alldata, alllabel, splitpercent)\n",
        "    valpred = classifier(traindata, trainlabel, valdata)\n",
        "    accuracy += Accuracy(vallabel, valpred)\n",
        "  return accuracy/iterations # average of all accuracies"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3qtNar7Bbik",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43766d3f-f37a-4985-f0b4-f7fc7a6d1998"
      },
      "source": [
        "print('Average validation accuracy is ', AverageAccuracy(alltraindata, alltrainlabel, 75/100, 10, classifier=NN))\n",
        "testpred = NN(alltraindata, alltrainlabel, testdata)\n",
        "print('test accuracy is ',Accuracy(testlabel, testpred) )"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average validation accuracy is  0.33584635395170215\n",
            "test accuracy is  0.34917953667953666\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "33GIn4x5VH-d"
      },
      "source": [
        "This is a very simple way of doing cross-validation. There are many well-known algorithms for cross-validation, like k-fold cross-validation, leave-one-out etc. This will be covered in detail in a later module. For more information about cross-validation, check <a href=https://en.wikipedia.org/wiki/Cross-validation_(statistics)>Cross-validatioin (Wikipedia)</a>\n",
        "\n",
        "### Questions\n",
        "1. Does averaging the validation accuracy across multiple splits give more consistent results?\n",
        "2. Does it give more accurate estimate of test accuracy?\n",
        "3. What is the effect of the number of iterations on the estimate? Do we get a better estimate with higher iterations?\n",
        "4. Consider the results you got for the previous questions. Can we deal with a very small train dataset or validation dataset by increasing the iterations?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER 1**"
      ],
      "metadata": {
        "id": "lTm12wBlUcML"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Yes, averaging the validation accuracy across multiple splits, typically in the context of cross-validation, can indeed provide more consistent and reliable results when evaluating the performance of a machine learning model. This approach is commonly used to obtain a more robust estimate of a model's generalization performance.\n",
        "\n",
        "Here's why averaging validation accuracy across multiple splits can be beneficial:\n",
        "\n",
        "Reduced Variance: When you split your dataset into multiple subsets (folds) and train/validate your model on each fold, you get multiple estimates of your model's performance. Averaging these estimates reduces the variance in your evaluation, making it less sensitive to the specific random data splits.\n",
        "\n",
        "Better Generalization Estimate: By testing your model on different subsets of the data, you obtain a more comprehensive assessment of how well it generalizes to unseen data. This helps ensure that your model's performance is not just good for one specific data partition but is indicative of its overall performance.\n",
        "\n",
        "Reduced Risk of Overfitting: Averaging across multiple splits can help identify if your model is overfitting or underfitting. Consistent performance across folds suggests better generalization, while significant variability may indicate overfitting or instability.\n",
        "\n",
        "Improved Confidence Intervals: You can compute confidence intervals or standard deviations on the averaged metrics, which provide insights into the level of confidence you can have in your model's performance estimate.\n",
        "\n",
        "Better Hyperparameter Tuning: Cross-validation can be used in hyperparameter tuning to find the best set of hyperparameters that lead to the most consistent and robust model performance.\n",
        "\n",
        "Common cross-validation techniques include k-fold cross-validation (where you split the data into k subsets and train/validate the model k times), stratified cross-validation (ensuring that class distributions are similar in each fold), and leave-one-out cross-validation (a special case where each data point serves as its own validation set).\n",
        "\n",
        "However, it's essential to keep in mind that cross-validation can be computationally expensive, especially for large datasets and complex models. It may not always be necessary, and sometimes a simple train-validation split is sufficient. The choice of cross-validation method and the number of splits (k) depends on your specific dataset and modeling goals.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "xMZhmJKoVlWY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ANSWER2**"
      ],
      "metadata": {
        "id": "PJq-STWaVnqm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Averaging the validation accuracy across multiple splits, such as in cross-validation, does not give a more accurate estimate of test accuracy. Instead, it provides a more accurate estimate of how well your model is likely to perform on unseen data, which is a measure of its generalization performance.\n",
        "\n",
        "Here's the distinction:\n",
        "\n",
        "Validation Accuracy: This is used to assess how well your model is performing on a portion of your dataset that it has not seen during training. The purpose is to tune hyperparameters, select the best model, and understand its behavior. It doesn't provide an accurate estimate of how your model will perform on completely new, unseen data.\n",
        "\n",
        "Test Accuracy: This is a measure of your model's performance on completely new and unseen data, which is typically separate from both the training and validation data. It is used to evaluate how well your model generalizes to real-world, out-of-sample data.\n",
        "\n",
        "Cross-validation, by averaging validation accuracy across multiple splits, provides a better estimate of how well your model generalizes to new data compared to a single validation split. However, it is still an estimate and not a direct measure of test accuracy on completely unseen data.\n",
        "\n"
      ],
      "metadata": {
        "id": "vCIjYit9VhQE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer3**"
      ],
      "metadata": {
        "id": "DrwKphCKWxAL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The number of iterations (or epochs) in the context of training a machine learning model can have an impact on the estimate of the model's performance, but it doesn't necessarily lead to a better estimate with higher iterations. The relationship between the number of iterations and the estimate of model performance is more nuanced and depends on various factors:\n",
        "\n",
        "Underfitting and Overfitting: The number of iterations is closely related to the model's capacity to fit the training data. If you have too few iterations, the model may underfit the data, leading to poor performance. If you have too many iterations, the model can overfit the data, performing well on the training data but poorly on new, unseen data. Therefore, the right number of iterations strikes a balance between underfitting and overfitting.\n",
        "\n",
        "Early Stopping: To find the optimal number of iterations, it's common practice to use techniques like early stopping. Early stopping monitors the model's performance on a validation set during training and stops training when performance starts to degrade. This prevents overfitting and can lead to a better estimate of how well the model will generalize.\n",
        "\n",
        "Training Data Size: The number of iterations can also depend on the size of your training data. With a smaller dataset, you may need fewer iterations, while a larger dataset may require more iterations to converge.\n",
        "\n",
        "Complexity of the Model: More complex models may require more iterations to learn the underlying patterns in the data. Simpler models might converge quickly with fewer iterations.\n",
        "\n",
        "Stochastic Variability: Some training algorithms have inherent stochasticity (e.g., stochastic gradient descent), and the estimate of model performance can vary from one training run to another, even with the same hyperparameters and dataset. Averaging results over multiple runs can help reduce this variability.\n",
        "\n",
        "Regularization: The use of regularization techniques, such as dropout or L1/L2 regularization, can affect the optimal number of iterations. Regularization helps control overfitting and can allow you to train for more iterations without overfitting.\n",
        "\n"
      ],
      "metadata": {
        "id": "UIkCnykEWDUV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**answer4**"
      ],
      "metadata": {
        "id": "L8q--prhW1ZV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Increasing the number of iterations can help to some extent when dealing with a very small training dataset, but it is not a guaranteed solution, and there are limitations to what it can achieve. Here are some considerations:\n",
        "\n",
        "Advantages of Increasing Iterations with a Small Training Dataset:\n",
        "\n",
        "Model Learning: With more iterations, the model has more opportunities to learn from the limited training data. This can help the model capture more complex patterns and potentially improve its performance.\n",
        "\n",
        "Avoiding Underfitting: In some cases, if the initial number of iterations is too low, the model may underfit the training data. Increasing iterations can mitigate underfitting to some extent.\n",
        "\n",
        "Limitations and Considerations:\n",
        "\n",
        "Overfitting Risk: Increasing the number of iterations significantly with a very small training dataset can lead to overfitting. The model may start fitting noise in the data, resulting in poor generalization to new, unseen data. It's crucial to monitor the model's performance on a validation set and use techniques like early stopping to prevent overfitting.\n",
        "\n",
        "Data Quality: If your training dataset is very small and not representative of the population you're interested in, increasing iterations may not help much. The model can only learn what is present in the data. If the data is noisy, unrepresentative, or contains biases, more iterations won't fundamentally improve the model's performance.\n",
        "\n",
        "Model Complexity: The complexity of the model matters. Very complex models may require a large amount of data to generalize effectively. Increasing iterations alone may not compensate for a lack of data.\n",
        "\n",
        "Regularization: If your dataset is small, it's often beneficial to use regularization techniques, such as dropout or L1/L2 regularization, to help control overfitting. These techniques can complement increased iterations by stabilizing the learning process.\n"
      ],
      "metadata": {
        "id": "W1o_aRN_WvFU"
      }
    }
  ]
}